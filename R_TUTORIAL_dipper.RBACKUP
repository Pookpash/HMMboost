# rsync -azr --progress /home/rob/Documents/school/Duke/machinelearn/HMMboost/* rob@134.115.238.47:/home/rob/Documents/school/Duke/machinelearn/HMMboost/
# rsync -azr --progress /home/rob/Documents/school/Duke/machinelearn/markboost/*SOURCE.R rob@134.115.238.47:/home/rob/Documents/school/Duke/machinelearn/markboost/
# rsync -azr --progress /home/rob/Documents/school/Duke/machinelearn/markboost/EM/R_CJSboost_SOURCE.R rob@134.115.238.47:/home/rob/Documents/school/Duke/machinelearn/markboost/EM/R_CJSboost_SOURCE.R
# rsync -azr --progress rob@134.115.238.47:/home/rob/Documents/school/Duke/machinelearn/HMMboost/PLOT_* /home/rob/Documents/school/Duke/machinelearn/HMMboost/

# Dipper analysis with CJSboosting (with trees, splines, and PLS base-learners) and compare to AICc model-averaging. This is example from Rankin 2016.

setwd("~/Documents/school/Duke/machinelearn/HMMboost")
library(Rcpp) # for C++ cjsboost functions
library(RcppArmadillo) # for C++ cjsboost functions
library(inline) # for C++ cjsboost functions
library(boot) # logit command
library(mboost) # base-learners for boosting
library(parallel) # need for parallel processing during the bootstrap-runs
library(party) # (optional) conditional inference trees (for "boosted regression trees")
#source("R_CJSboost_SOURCE.R") # import CJSBoost functions (may take several seconds)
source("../markboost/EM/R_CJSboost_SOURCE.R") # import CJSBoost functions (may take several seconds)
source("../markboost/R_simcjsdataset_SOURCE.R") # import stupid simulation functions ( optional)


# import the dipper data (from RMark and Lebreton ????)
dipper <- read.table(file="data_dipper_lebreton.txt",header=TRUE,stringsAsFactors=FALSE,colClasses=c("character"))
# covert the dipper data from MARK format to R matrix
ch.data.wide <- ch2wide(ch=dipper,col=1,col.stem="") # matrix of capture histories
T = ncol(ch.data.wide) # number of capture periods
# WARNING: make sure to remove ALL capture histories WITHOUT obserations in 1:(T-1), 
first_ <- first(wide=ch.data.wide[,1:T]) # find first observation
keep <- which(first_<T) # which observations to keep (i.e., seen at least once before T)
y <- ch.data.wide[keep,] # only keep individuals whose first capture was BEFORE T
# covariate data: individual-level covariates 
cov.dat.indiv <- data.frame(dippid = as.factor(1:nrow(ch.data.wide)),sex = as.factor(ifelse(dipper[,"sex"]=="Female",0,1)))[keep,]
# covariate data: time-varying (and possibly individual varying); need to input as an array
cov.dat.array <- array(0,c(nrow(y),ncol(y),2),dimnames=list(row.names(y),1:T,c("floods","floodp")))
cov.dat.array[,,"floods"] <- (rep(1,nrow(y)))%x%t(c(0,0,1,1,0,0,0)) # FLOOD periods are during capture period 3,4 (for surivival)
cov.dat.array[,,"floodp"] <- (rep(1,nrow(y)))%x%t(c(0,0,1,0,0,0,0)) # FLOOD periods are during capture period 3 (for observation)

# Bootstrap weights: generate now, so that we can compare different formulas (bols, splines, trees)
N_bootstrap=30 # in paper, I use 70
bootstrap = subsampF(labels=1:nrow(y), ntimes=N_bootstrap, method="bootstrap") # subsampling function 
bootstrap_weights <- lapply(bootstrap, function(x) x$inbag) # weights used for training: holdout set are the zeros

####################################################################################################
# PART 1: Least-Squares Base-learners (penalized and ordinary least-squares)
# includes MAIN effects for timefactor, flood and sex; and their two-ways interactions
# NOTE: the higher-order interactions must be CONSTRAINED with df=1
form.name = "model.PLS" # name of model
formu.PLS = list( # formula is a NAMED-LIST, with an R formula entry for 's' surival and 'p' capture prob
    s=~bols(interc,intercept=FALSE)+bols(floods)+bols(timefactor,df=1)+bols(sex)+bols(floods,sex,df=1)+bols(floods,by=sex,df=1)+bols(timefactor,by=sex,df=1),
    p=~bols(interc,intercept=FALSE)+bols(floodp)+bols(timefactor,df=1)+bols(sex)+bols(floodp,sex,df=1)+bols(floodp,by=sex,df=1)+bols(timefactor,by=sex,df=1)
    ) 

# Hyperparameters tuning: we must optimize the hyperparameters 'nu' and 'mstop'. mstop is the total number of boosting iterations. nu is the shrinkage rate. The following function runs NU.STEP bootstrap-validations trying to find a reasonable pair of 'nu' for survival and 'p'. Then we tune 'mstop'
nu.start = list(s=0.02,p=0.01) # initial shrinkage rates (named list)
mstop.cv = 1000 # stopping iteration (should be overly generous)
m_estw <- 2 # integer, how often to re-calculate the E-step? (1-3) smaller is more computationally expensive
mc.cores = 10 # number of threads for parallel processing; used in parallel::mcLapply

# BOOTSTRAP-VALIDATION: for PLS baselearners
cv.PLS <- cjsboost_hyperparam( formula=formu.PLS, # named list of R formula's (response not necessary)
    ch.data=y,  # capture-histories in matrix format
    cov.data.wide=cov.dat.indiv,  # data.frame  individual level covariates
    cov.data.array=cov.dat.array, # array for covariates that vary by time (and maybe individual)
    mstop = mstop.cv, # stopping criteria, 
    m_estw=m_estw,# how often to perform E-step? (every m_estw'th of boosting iteratn)
    nu.start=nu.start, # named list of the shrinkage rate, for different shrinkage per component (named the same as in formula)
    nu_search_steps = 7, # how many steps to take to find an optimal nu
    offsets=NA, # named list of start values per component (named the same as in formula) NA = MLE of phi(dot)p(dot)
    add.intercept=TRUE, # option to automatically add an intercept variable called 'interc'
    id = NULL, # optional vector of IDs to identify rows in data with ch data 
    N_bootstrap=N_bootstrap,# number of bootstrap iteration
    mc.cores=mc.cores, # package(parallel): parallelize the bootstrap runs, number of cores 
    bootstrap_weights=bootstrap_weights, # option to patch in your own weights,
    return_all_cv_models=FALSE) #  return ALL the cvmods (not a good idea); otherwise, just return the CV runs for the optimal nu (defa
print(cv.PLS$nu.optimization.summary) # look at bestm, cvrisk. We want bestm and nu to optimize cvrisk

bestm.PLS <- round(cv.PLS$bestm$median) # best m
bestnu.PLS <-  cv.PLS$optimal.nu # best nu
expected.risk.PLS <- mean(cv.PLS$cvrisk.rescale[bestm.PLS,]) # expected risk
# PLS final model for inference: run until bestm
mod.PLS <- cjsboost(formula=formu.PLS, ch.data=y, cov.data.wide=cov.dat.indiv, cov.data.array=cov.dat.array, mstop = bestm.PLS, m_estw=m_estw, nu=bestnu.PLS, add.intercept=TRUE)
# PLS: done model

# PLS: print which base-learners were selected most (per s and p)
print(mod.PLS$summary)

# PLS: print the (shrunken) estimates of the coefficients
coefs <- boost_coef(mod.PLS)
print(coefs[["s"]])
print(coefs[["p"]])

# PLS: plot the gradient descent (show the progress of the risk minimization, per hold-out set)
par(mfrow=c(1,2))
plot(c(1,mstop.cv),range(cv.PLS$cvrisk.rescale),typ="n",xlab="Boosting Iteration (m)",ylab="Holdout Risk")
for(j in 1:N_bootstrap){ lines(cv.PLS$cvrisk.rescale[,j],col=grey(runif(1,0.2,0.8)))} # each subsample hold-out risk descent
expected.risk <- rowMeans(cv.PLS$cvrisk.rescale) # calculate our estimate of the expected risk
lines(expected.risk,lwd=3) # plot expected risk (what we actually want to minimize)
abline(v=bestm.PLS,col="red",lty=2) # plot optimal m
plot(c(1,mstop.cv),range(mod.PLS$risk),typ="n",xlab="Boosting Iteration (m)",ylab="Regularized Risk") # empirical
lines(mod.PLS$risk); abline(v=bestm.PLS,col="red",lty=2)

# PLS: plot the coefficients, comparing sex x time
plot.data <- cbind(data.frame(floods = rep(c(0,1,1,0,0,0),2),floodp=rep(c(0,1,0,0,0,0),2)),expand.grid(timefactor=factor(c(2:(T)),levels=1:(T)), sex=factor(c(1,0)))) # new data for plotting the effects))
preds.PLS.list = boost_predict(mod.PLS,plot.data) # PLS boost estimates (on logit scale)
preds.PLS = cbind(plot.data, p = inv.logit(preds.PLS.list$p), s = inv.logit(preds.PLS.list$s)) # back-transform estimates to the probability scale
preds.PLS$period = as.numeric(preds.PLS$timefactor) # make it numeric, for eacy comparison
par(mfrow=c(2,2),bty="n");
plot(subset(preds.PLS,sex==0)[,c("period","s")],xlab="capture period",main="survival females (PLS)",ylab="survival",typ="b")
plot(subset(preds.PLS,sex==1)[,c("period","s")],xlab="capture period",main="survival males (PLS)",ylab="survival",typ="b")
plot(subset(preds.PLS,sex==0)[,c("period","p")],xlab="capture period",main="detection females (PLS)",ylab="capture prob",typ="b")
plot(subset(preds.PLS,sex==1)[,c("period","p")],xlab="capture period",main="detection males (PLS)",ylab="capture prob",typ="b")
# done for now Penalized least squares

####################################################################################################
# PART 2: Spline on time Base-learners (instead of time-as-a-factor)
# includes MAIN effects for time, flood and sex; and their two-ways interactions
# NOTE: the higher-order interactions must be CONSTRAINED with df=1 (see how to constrain and equalize the degrees-of-freedom of baselearners by reading "Kneib, T., Hothorn, T., & Tutz, G. (2009). Variable selection and model choice in geoadditive regression models. Biometrics, 65(2), 626â€“634. DOI:10.1111/j.1541-0420.2008.01112.x"
form.name = "model.SPLINE" # name of model
formu.SPLINE = list( # formula is a NAMED-LIST, with an R formula entry for 's' surival and 'p' capture prob
    s=~bols(interc,intercept=FALSE) + bols(floods) + bols(time) + bols(sex) + bols(floods,sex,df=1) + bols(floods,by=sex,df=1) + bols(time,by=sex,df=1) + bbs(time,knots=6,df=1,center=TRUE) + bbs(time,by=sex,knots=6,df=1,center=TRUE),
    p=~bols(interc,intercept=FALSE) + bols(floodp) + bols(time) + bols(sex) + bols(floodp,sex,df=1) + bols(floodp,by=sex,df=1) + bbs(time,knots=6,df=1,center=TRUE) + bbs(time,by=sex,knots=6,df=1,center=TRUE)
)

# Hyperparameters tuning: we must optimize the hyperparameters 'nu' and 'mstop'. mstop is the total number of boosting iterations. nu is the shrinkage rate. The following function runs NU.STEP bootstrap-validations trying to find a reasonable pair of 'nu' for survival and 'p'. Then we tune 'mstop'
# nu.start = list(s=0.2,p=0.1) # initial shrinkage rates (named list)
nu.start = bestnu.PLS # CHEATING: starting with the best estimate of nu from the PLS (just to save some time)
mstop.cv = 1000 # stopping iteration (should be overly generous)
m_estw <- 2 # integer, how often to re-calculate the E-step? (1-3) smaller is more computationally expensive

# BOOTSTRAP-VALIDATION: for PLS baselearners
cv.SPLINE <- cjsboost_hyperparam( formula=formu.SPLINE, # named list of R formula's (response not necessary)
    ch.data=y,  # capture-histories in matrix format
    cov.data.wide=cov.dat.indiv,  # data.frame  individual level covariates
    cov.data.array=cov.dat.array, # array for covariates that vary by time (and maybe individual)
    mstop = mstop.cv, # stopping criteria, 
    m_estw=m_estw,# how often to perform E-step? (every m_estw'th of boosting iteratn)
    nu.start=nu.start, # named list of the shrinkage rate, for different shrinkage per component (named the same as in formula)
    nu_search_steps = 2, # how many steps to take to find an optimal nu
    offsets=NA, # named list of start values per component (named the same as in formula) NA = MLE of phi(dot)p(dot)
    add.intercept=TRUE, # option to automatically add an intercept variable called 'interc'
    id = NULL, # optional vector of IDs to identify rows in data with ch data 
    N_bootstrap=N_bootstrap,# number of bootstrap iteration
    mc.cores=mc.cores, # package(parallel): parallelize the bootstrap runs, number of cores 
    bootstrap_weights=bootstrap_weights, # option to patch in your own weights,
    return_all_cv_models=FALSE) #  return ALL the cvmods (not a good idea); otherwise, just return the CV runs for the optimal nu (def

print(cv.SPLINE$nu.optimization.summary) # look at bestm, cvrisk. We want bestm and nu to optimize cvrisk

bestm.SPLINE <- round(cv.SPLINE$bestm$median) # best m
bestnu.SPLINE <-  cv.SPLINE$optimal.nu # best nu
expected.risk.SPLINE <- mean(cv.SPLINE$cvrisk.rescale[bestm.SPLINE,]) # expected risk
# SPLINE final model for inference: run until bestm
mod.SPLINE <- cjsboost(formula=formu.SPLINE, ch.data=y, cov.data.wide=cov.dat.indiv, cov.data.array=cov.dat.array, mstop = bestm.SPLINE, m_estw=m_estw, nu=bestnu.SPLINE, add.intercept=TRUE)
# SPLINE: done model

# SPLINE: print which base-learners were selected most (per s and p)
print(mod.SPLINE$summary)

# SPLINE: print the (shrunken) estimates of the coefficients
coefs <- boost_coef(mod.SPLINE)
print(coefs[["s"]])
print(coefs[["p"]])

# SPLINE: plot the coefficients, comparing sex x time
time.scaling <- mod.SPLINE$time.scaling # get the scaled/continuous time values 
plot.data <- data.frame(period = rep(time.scaling$t,2), time = rep(time.scaling$covariate,2), floods = rep(c(0,1,1,0,0,0),2), floodp=rep(c(0,1,0,0,0,0),2), sex = rep(factor(c(0,1)), each = 6))
preds.SPLINE.list = boost_predict(mod.SPLINE,plot.data) # SPLINE boost estimates (on logit scale)
preds.SPLINE = cbind(plot.data, p = inv.logit(preds.SPLINE.list$p), s = inv.logit(preds.SPLINE.list$s)) # back-transform estimates to the probability scale
par(mfrow=c(2,2),bty="n");
plot(subset(preds.SPLINE,sex==0)[,c("period","s")],xlab="capture period",main="survival females (SPLINE)",ylab="survival",typ="b")
plot(subset(preds.SPLINE,sex==1)[,c("period","s")],xlab="capture period",main="survival males (SPLINE)",ylab="survival",typ="b")
plot(subset(preds.SPLINE,sex==0)[,c("period","p")],xlab="capture period",main="detection females (SPLINE)",ylab="capture prob",typ="b")
plot(subset(preds.SPLINE,sex==1)[,c("period","p")],xlab="capture period",main="detection males (SPLINE)",ylab="capture prob",typ="b")

####################################################################################################
# PART 2: CART-like conditional inference trees (algorithmic inference; like boosted regression trees)
# just ONE base-learner (per p,s): let the trees fit and variable select!
form.name = "model.TREES" # name of model
formu.TREES = list( # formula is a NAMED-LIST, with an R formula entry for 's' surival and 'p' capture prob
    s=~bols(interc,intercept=FALSE)+btree(timefactor,sex,floods,tree_controls=party::ctree_control(maxdepth=2,mincriterion=0, savesplitstats = FALSE)),
    p=~bols(interc,intercept=FALSE)+btree(timefactor,sex,floodp,tree_controls=party::ctree_control(maxdepth=2,mincriterion=0, savesplitstats = FALSE))
    )
        
# Hyperparameters tuning: we must optimize the hyperparameters 'nu' and 'mstop'. mstop is the total number of boosting iterations. nu is the shrinkage rate. The following function runs NU.STEP bootstrap-validations trying to find a reasonable pair of 'nu' for survival and 'p'. Then we tune 'mstop'
# nu.start = list(s=0.2,p=0.1) # initial shrinkage rates (named list)
nu.start = bestnu.PLS # CHEATING: using the estimating bestnu from PLS to initialize
mstop.cv = 1000 # stopping iteration (should be overly generous)
,m_estw <- 2 # integer, how often to re-calculate the E-step? (1-3) smaller is more computationally expensive

# BOOTSTRAP-VALIDATION: for TREES baselearners
cv.TREES <- cjsboost_hyperparam( formula=formu.TREES, # named list of R formula's (response not necessary)
    ch.data=y,  # capture-histories in matrix format
    cov.data.wide=cov.dat.indiv,  # data.frame  individual level covariates
    cov.data.array=cov.dat.array, # array for covariates that vary by time (and maybe individual)
    mstop = mstop.cv, # stopping criteria, 
    m_estw=m_estw,# how often to perform E-step? (every m_estw'th of boosting iteratn)
    nu.start=nu.start, # named list of the shrinkage rate, for different shrinkage per component (named the same as in formula)
    nu_search_steps = 2, # how many steps to take to find an optimal nu
    offsets=NA, # named list of start values per component (named the same as in formula) NA = MLE of phi(dot)p(dot)
    add.intercept=TRUE, # option to automatically add an intercept variable called 'interc'
    id = NULL, # optional vector of IDs to identify rows in data with ch data 
    N_bootstrap=N_bootstrap,# number of bootstrap iteration
    mc.cores=mc.cores, # package(parallel): parallelize the bootstrap runs, number of cores 
    bootstrap_weights=bootstrap_weights, # option to patch in your own weights,
    return_all_cv_models=FALSE) #  return ALL the cvmods (not a good idea); otherwise, just return the CV runs for the optimal nu (defa

bestm.TREES <- round(cv.TREES$bestm$median) # best m
bestnu.TREES <-  cv.TREES$optimal.nu # best nu
expected.risk.TREES <- mean(cv.TREES$cvrisk.rescale[bestm.TREES,]) # expected risk
# TREES final model for inference: run until bestm
mod.TREES <- cjsboost(formula=formu.TREES, ch.data=y, cov.data.wide=cov.dat.indiv, cov.data.array=cov.dat.array, mstop = bestm.TREES, m_estw=m_estw, nu=bestnu.TREES, add.intercept=TRUE)
# TREES: done model

# TREES: print which base-learners were selected most (per s and p)
print(mod.TREES$summary) # point-less (because we only really had one btree)
# ... more meaningful to look at the 'selected_in_ensemble' statistiics
sel.ens <- selected_in_ensemble(mod.TREES) # extra which covariates were selected PER tree
table(unlist(sel.ens[["s"]]))
table(unlist(sel.ens[["p"]]))

# TREES: no coefficients for trees (only trees!) so, do NOT run the following
# coefs <- boost_coef(mod.TREES)

# TREES: plot the coefficients, comparing sex x time
plot.data <- cbind(data.frame(floods = rep(c(0,1,1,0,0,0),2),floodp=rep(c(0,1,0,0,0,0),2)),expand.grid(timefactor=factor(c(2:(T)),levels=1:(T)), sex=factor(c(1,0)))) # new data for plotting the effects))
preds.TREES.list = boost_predict(mod.TREES,plot.data) # TREES boost estimates (on logit scale)
preds.TREES = cbind(plot.data, p = inv.logit(preds.TREES.list$p), s = inv.logit(preds.TREES.list$s)) # back-transform estimates to the probability scale
preds.TREES$period = as.numeric(preds.TREES$timefactor) # make it numeric, for eacy comparison
par(mfrow=c(2,2),bty="n");
plot(subset(preds.TREES,sex==0)[,c("period","s")],xlab="capture period",main="survival females (TREES)",ylab="survival",typ="b")
plot(subset(preds.TREES,sex==1)[,c("period","s")],xlab="capture period",main="survival males (TREES)",ylab="survival",typ="b")
plot(subset(preds.TREES,sex==0)[,c("period","p")],xlab="capture period",main="detection females (TREES)",ylab="capture prob",typ="b")
plot(subset(preds.TREES,sex==1)[,c("period","p")],xlab="capture period",main="detection males (TREES)",ylab="capture prob",typ="b")
# done for now

####################################################################################
# PART 4: COMPARE boosted estimates: PLS vs. Splines vs. Trees (visually)

png(filename="PLOT_comparison1.png",width=700,height=700,pointsize=18)
par(mfrow=c(2,2),bty="n");
for(par_ in c("s","p")){
    for(sex_ in c(0,1)){
        rangey = range(subset(preds.PLS,sex==sex_)[,par_],subset(preds.SPLINE,sex==sex_)[,par_],subset(preds.TREES,sex==sex_)[,par_])
        plot(c(2,T), rangey,typ="n",xlab="capture period",ylab=c(s="survival",p="capture probability")[[par_]],main =c("female","male") [sex_+1]) # blank plot
        lines(subset(preds.PLS,sex==sex_)[,c("period",par_)],typ="b",pch=1,col="orange")
        lines(subset(preds.SPLINE,sex==sex_)[,c("period",par_)],typ="b",pch=6,col="purple")
        lines(subset(preds.TREES,sex==sex_)[,c("period",par_)],typ="b",pch=18,col="green")
    } # sex
} # parameter
legend(x="bottomright",legend=c("boost-PLS","boost-Splines","boost-Trees"),col=c("orange","purple","green"),lwd=c(1,1,1),pch=c(1,6,18),bty="n")
dev.off()


###########################################################################################
# PART 5: COMPARE CJSboost to Maximum-Likelihood Estimates and AICc-Model-averaging (Rmark)
# AICc is also based on a predictive criteria, minimax optimal in expected loss
# WARNING: REQUIRES RMark and Program Mark (cross-platform, free, but NOT open-source)
library(RMark)
if( "RMark" %in% row.names(installed.packages())){ # check if YOU have RMark installed
    data(dipper) # load dipper data
    dipper.processed=process.data(dipper,groups=("sex"))
    dipper.ddl=make.design.data(dipper.processed)
    dipper.ddl$Phi$Flood=0 # RMark way of adding FLood covariate
    dipper.ddl$Phi$Flood[dipper.ddl$Phi$time==2 | dipper.ddl$Phi$time==3]=1
    dipper.ddl$p$Flood=0; dipper.ddl$p$Flood[dipper.ddl$p$time==3]=1
    dipper.ddl$p$ctime = dipper.ddl$p$time ; dipper.ddl$Phi$ctime = dipper.ddl$Phi$time
    dipper.ddl$p$ctime[which(dipper.ddl$p$time==T)]<-(T-1); dipper.ddl$Phi$ctime[which(dipper.ddl$Phi$time==(T-1))]<-(T-2)
    # RMark: all the different specifications
    Phidot=list(formula=~1)
    Phitime=list(formula=~time)
    Phitimec=list(formula=~ctime)
    Phisex=list(formula=~sex)
    Phisextime=list(formula=~sex+time)
    Phisextimec=list(formula=~sex+ctime)
    Phisex.time=list(formula=~sex*time)
    Phisex.timec=list(formula=~sex*ctime)
    PhiFlood=list(formula=~Flood)
    PhiFloodsex=list(formula=~Flood+sex)
    PhiFlood.sex=list(formula=~Flood*sex)
    pdot=list(formula=~1)
    ptime=list(formula=~time)
    ptimec=list(formula=~ctime)
    psex=list(formula=~sex)
    psextimec=list(formula=~sex+ctime)
    psextime=list(formula=~sex+time)
    psex.time=list(formula=~sex*time)
    psex.timec=list(formula=~sex*ctime)
    pFlood=list(formula=~Flood)
    pFloodsex=list(formula=~Flood+sex)
    pFlood.sex=list(formula=~Flood*sex)
    # RMark: run all the 64 possible fixed effects models
    # phidot
    
    dipper.phidot.pdot=mark(dipper.processed,dipper.ddl, model.parameters=list(Phi=Phidot,p=pdot),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phidot.psex =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phidot,p=psex),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phidot.ptime =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phidot,p=ptime),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phidot.psextime =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phidot,p=psextime),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phidot.psex.time =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phidot,p=psex.timec),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phid.pFlood =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phidot,p=pFlood),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phid.pFloodsex =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phidot,p=pFloodsex),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phid.pFlood.sex =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phidot,p=pFlood.sex),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    # phitime
    dipper.phitime.ptime =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phitime, p=ptimec),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phitime.pdot =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phitime,p=pdot),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phitime.psex =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phitime,p=psex),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phitime.psextime =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phitime,p=psextimec),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phitime.psex.time =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phitime,p=psex.timec),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phitime.pFlood=mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phitime,p=pFlood),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores ,group="sex",delete=TRUE)
    dipper.phitime.pFloodsex =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phitime,p=pFloodsex),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phitime.pFlood.sex =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phitimec,p=pFlood.sex),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
                                        # phisex
    dipper.phisex.pdot =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phisex,p=pdot),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phisex.psex =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phisex,p=psex),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phisex.psextime =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phisex,p=psextime),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phisex.psex.time =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phisex,p=psex.time),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phisex.ptime =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phisex,p=ptime),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phisex.pFlood=mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phisex,p=pFlood),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phisex.pFloodsex =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phisex,p=pFloodsex),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phisex.pFlood.sex =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phisex,p=pFlood.sex),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
                                        # phisextime
    dipper.phisextime.pdot =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phisextime,p=pdot),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phisextime.ptime =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phisextime,p=ptimec),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phisextime.psex =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phisextime,p=psex),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phisextime.psextime =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phisextimec,p=psextime),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phisextime.psex.time =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phisextimec,p=psex.timec),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phisextime.pFlood =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phisextime,p=pFlood),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phisextime.pFloodsex =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phisextime,p=pFloodsex),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phisextime.pFood.sex =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phisextimec,p=pFlood.sex),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
                                        # phisex.time
    dipper.phisex.time.pdot =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phisex.time,p=pdot),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phisex.time.ptime =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phisex.time,p=ptimec),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phisex.time.psex =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phisex.time,p=psex),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phisex.time.psextime =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phisex.timec,p=psextime),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phisex.time.psex.time =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phisex.timec,p=psex.timec),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phisex.time.pFlood =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phisex.time,p=pFlood),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phisex.time.pFloodsex =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phisex.time,p=pFloodsex),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phisex.time.pFood.sex =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=Phisex.timec,p=pFlood.sex),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
                                        # phi flood
    dipper.phiFlood.pd=mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=PhiFlood,p=pdot),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phiFlood.ptime=mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=PhiFlood,p=ptimec),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phiFlood.psex=mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=PhiFlood,p=psex),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phiFlood.psextime=mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=PhiFlood,p=psextimec),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phiFlood.psex.time=mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=PhiFlood,p=psex.timec),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores ,group="sex",delete=TRUE)
    dipper.phiFlood.pFlood=mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=PhiFlood,p=pFlood),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phiFlood.pFloodsex =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=PhiFlood,p=pFloodsex),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phiFlood.pFlood.sex =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=PhiFlood,p=pFlood.sex),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
                                        # phifloodsex
    dipper.phiFloodsex.pd=mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=PhiFloodsex,p=pdot),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phiFloodsex.ptime=mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=PhiFloodsex,p=ptimec),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phiFloodsex.psex=mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=PhiFloodsex,p=psex),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phiFloodsex.psextime=mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=PhiFloodsex,p=psextimec),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phiFloodsex.psex.time=mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=PhiFloodsex,p=psex.timec),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores ,group="sex",delete=TRUE)
    dipper.phiFloodsex.pFlood=mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=PhiFloodsex,p=pFlood),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phiFloodsex.pFloodsex =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=PhiFloodsex,p=pFloodsex),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phiFloodsex.pFlood.sex =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=PhiFloodsex,p=pFlood.sex),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    # phiflood.sex
    dipper.phiFlood.sex.pd=mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=PhiFlood.sex,p=pdot),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phiFlood.sex.ptime=mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=PhiFlood.sex,p=ptimec),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phiFlood.sex.psex=mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=PhiFlood.sex,p=psex),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phiFlood.sex.psextime=mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=PhiFlood.sex,p=psextimec),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phiFlood.sex.psex.time=mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=PhiFlood.sex,p=psex.timec),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores ,group="sex",delete=TRUE)
    dipper.phiFlood.sex.pFlood=mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=PhiFlood.sex,p=pFlood),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phiFlood.sex.pFloodsex =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=PhiFlood.sex,p=pFloodsex),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    dipper.phiFlood.sex.pFlood.sex =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=PhiFlood.sex,p=pFlood.sex),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,group="sex",delete=TRUE)
    # collect all the mark models (from your memory)
    dipp.res <- lapply(as.list(ls(pattern="dipper\\.phi[pdotFloodsextime]")),FUN=get) # collect all R model objects named 'markxxx' # 25 models
    names(dipp.res) <- ls(pattern="dipper\\.phi[pdotFloodsextime]") # get model.names
    dipp.res <- c(dipp.res, list(model.table =model.table(model.list=dipp.res, type="CJS"))) # manually build 'marklist' for model.avg
    dipp.res[[length(dipp.res)]] # print the results
    class(dipp.res)<- "marklist" # need to make a 'marklist' so RMark can model-average
    # MODEL-AVERAGING
    mmodavg.raw <- model.average(dipp.res,vcv=TRUE) # model average: difficult to understand results...
    mark.indices = rbind(cbind(data.frame(parameter = rep("s",length(which(dipper.ddl$Phi$cohort==1)))),dipper.ddl$Phi[which(dipper.ddl$Phi$cohort==1),c("time","sex","model.index")]), cbind(data.frame(parameter = rep("p",length(which(dipper.ddl$p$cohort==1)))),dipper.ddl$p[which(dipper.ddl$p$cohort==1),c("time","sex","model.index")])) # get the 'model indices' from RMark
    mmodavg.est <- cbind(mark.indices, data.frame(hat=mmodavg.raw$estimate[mark.indices$model.index,"estimate"] ,lcl=mmodavg.raw$estimate[mark.indices$model.index,"lcl"],ucl=mmodavg.raw$estimate[mark.indices$model.index,"ucl"])) # append the model
    mmodavg.est <- mmodavg.est[order(mmodavg.est$parameter,mmodavg.est$sex,mmodavg.est$time),]
    print(mmodavg.est)
    # DONE with Rmark AICc model-averaging

    # MLE: get estimates from 'full-model' phi(t)p(t) by Maximum-Likelihood (run in RMark)
    mle.model =mark(dipper.processed,dipper.ddl,model.parameters=list(Phi=list(formula=~sex*ctime),p=list(formula=~sex*ctime)),output=FALSE,invisible=TRUE,silent=TRUE,retry=30,threads=mc.cores,delete=TRUE) # notice constraints on pht(T)=phi(T-1), p(T)=p(T-1)
    # trouble converging
    mle <- cbind(data.frame(parameter=gsub("p\\s.{0,}","p",gsub("Phi\\s.{0,}","s",row.names(mle.model$results$real),ignore.case=TRUE),ignore.case=TRUE), time = as.numeric(gsub("t","",regmatches(row.names(mle.model$results$real),regexpr("t\\d$", row.names(mle.model$results$real),perl=TRUE)))), sex = regmatches(row.names(mle.model$results$real),regexpr("(Male)|(Female)", row.names(mle.model$results$real),perl=TRUE))),mle.model$results$real[,c("estimate","lcl","ucl")]) # FOR THE DIPPER DATASET
    # because of constraints on T=T-1, need to manually at estimates at T
    mle <- rbind(mle, transform(subset(mle, (parameter=="s" & time==max(T-2)) | (parameter=="p" & time==max(T-1))),time=time+1)) 
    mle <- mle[order(mle$parameter,mle$sex,mle$time),]
    print(mle)
    # DONE MLE of full-model in RMark
    # file.remove(dir(pattern="mark[0-9]{3,}\\.[ceinprstuv]{0,}")) # dangerous command to delete all mark-model saved files
    # file.remove(dir(pattern="markxxx"))
    
    # shift time-period for "phi" in Mark (offset by 1 for survival, compared to boosting, i.e.,  1:6 vs 2:7)
    mmodavg.est$period = ifelse(mmodavg.est$parameter=="s",as.numeric(mmodavg.est$time)+1,as.numeric(mmodavg.est$time))
    mle$period = ifelse(mle$parameter=="s",as.numeric(mle$time)+1,as.numeric(mle$time))

    # PLOT: compare AICc vs boost-PLS vs boost-Spline vs boost-Trees
    png(filename="PLOT_comparison2.png",width=700,height=700,pointsize=18)
    par(mfrow=c(2,2),bty="n",mar=c(3.5,3.5,1,0),mgp=c(1.9,0.8,0));
    for(par_ in c("s","p")){
        for(sex_ in c(0,1)){
            rangey = range(subset(preds.PLS,sex==sex_)[,par_],subset(preds.SPLINE,sex==sex_)[,par_],subset(preds.TREES,sex==sex_)[,par_],subset(mle,sex==c("Female","Male")[sex_+1] & parameter==par_,"estimate"), subset(mmodavg.est,sex==c("Female","Male")[sex_+1] & parameter==par_,"hat")) # range of y-values
            plot(c(2,T), rangey,typ="n",xlab="capture period",ylab=c(s="survival",p="capture probability")[[par_]],main =c("female","male") [sex_+1]) # blank plot
            # CJSboosting models
            lines(subset(preds.PLS,sex==sex_)[,c("period",par_)],typ="b",pch=1,col="orange") # PLS
            lines(subset(preds.SPLINE,sex==sex_)[,c("period",par_)],typ="b",pch=1,col="purple") # Splines
            lines(subset(preds.TREES,sex==sex_)[,c("period",par_)],typ="b",pch=1,col="green") # Trees
            # AICc model-averaging
            lines(subset(mmodavg.est,sex==c("Female","Male")[sex_+1] & parameter==par_)[,c("period","hat")],col="blue",pch=18,typ="b")
            lines(subset(mle,sex==c("Female","Male")[sex_+1] & parameter==par_)[,c("period","estimate")],col="black",pch=19,typ="b")
            # MLE
        } # sex
    } # parameter
    legend(x="bottomright",legend=c("boost-PLS","boost-Splines","boost-Trees","model-averaged (AICc)",expression("MLE"~hat(phi)(t*scriptstyle(x)*"sex")*hat(p)(t*scriptstyle(x)*"sex"))),col=c("orange","purple","green","blue","black"),lwd=c(1,1,1,1,1),pch=c(1,1,1,18,19),bty="n")
    dev.off()
}
# Done comparing CJSboost vs AIC vs MLE

    
####################################################################################################
# PART 6: 'High-Dimensional' Example (with faked Dipper data)
# Here, we can some fake covariates to the dipper data and see how CJSboost does at shrinking their estimates to zero
dipper <- read.table(file="data_dipper_lebreton.txt",header=TRUE,stringsAsFactors=FALSE,colClasses=c("character")) # covert the dipper data from MARK format to R matrix
ch.data.wide <- ch2wide(ch=dipper,col=1,col.stem="") # matrix of capture histories
T = ncol(ch.data.wide) # number of capture periods
# WARNING: make sure to remove ALL capture histories WITHOUT obserations in 1:(T-1), 
first_ <- first(wide=ch.data.wide[,1:T]) # find first observation
keep <- which(first_<T) # which observations to keep (i.e., seen at least once before T)
y <- ch.data.wide[keep,] # only keep individuals whose first capture was BEFORE T

# covariate data: (individual-level AND then time-varying)
fake.dat.indiv <- data.frame(dippid = as.factor(1:nrow(ch.data.wide)),sex = as.factor(ifelse(dipper[,"sex"]=="Female",0,1)))[keep,]
fake.dat.array <- array(0,c(nrow(y),ncol(y),4),dimnames=list(row.names(y),1:T,c("floods","floodp","fake4","fake5")))
fake.dat.array[,,"floods"] <- (rep(1,nrow(y)))%x%t(c(0,0,1,1,0,0,0)) # FLOOD periods are during capture period 3,4 (for surivival)
fake.dat.array[,,"floodp"] <- (rep(1,nrow(y)))%x%t(c(0,0,1,0,0,0,0)) # FLOOD periods are during capture period 3 (for observation)

# fake covariate data
set.seed(1689)
n <- nrow(cov.dat.indiv)
fake.dat.indiv <- cbind(fake.dat.indiv,data.frame(fake1=rnorm(n),fake2=runif(n,-2,2), fake3=rnorm(n))) # 3 variables at the individual level
fake.dat.array[,,"fake4"] <- (rep(1,nrow(y)))%x%t(rnorm(T))
fake.dat.array[,,"fake5"] <- (rep(1,nrow(y)))%x%t(rnorm(T))

# PLS formula: bols on the fake covariates
formu.fake.PLS = list( # formula is a NAMED-LIST, with an R formula entry for 's' surival and 'p' capture prob
    s=~bols(interc,intercept=FALSE)+bols(floods)+bols(timefactor,df=1)+bols(sex)+bols(floods,sex,df=1)+bols(floods,by=sex,df=1)+bols(timefactor,by=sex,df=1) + bols(fake1,df=1) + bols(fake2,df=1) +bols(fake3,df=1)+bols(fake4,df=1) +bols(fake5,df=1), 
    p=~bols(interc,intercept=FALSE)+bols(floodp)+bols(timefactor,df=1)+bols(sex)+bols(floodp,sex,df=1)+bols(floodp,by=sex,df=1)+bols(timefactor,by=sex,df=1) + bols(fake1,df=1) + bols(fake2,df=1) +bols(fake3,df=1)+bols(fake4,df=1) +bols(fake5,df=1)
    ) 

nu.start = list(s=0.01,p=0.005) 
mstop.cv = 1000 # stopping iteration (should be overly generous)
m_estw <- 2 # integer, how often to re-calculate the E-step? (1-3) smaller is more computationally expensive

# BOOTSTRAP-VALIDATION: for PLS baselearners
cv.fake.PLS <- cjsboost_hyperparam( formula=formu.fake.PLS, # named list of R formula's (response not necessary)
    ch.data=y,  # capture-histories in matrix format
    cov.data.wide=fake.dat.indiv,  # data.frame  individual level covariates
    cov.data.array=fake.dat.array, # array for covariates that vary by time (and maybe individual)
    mstop = mstop.cv, # stopping criteria, 
    m_estw=m_estw,# how often to perform E-step? (every m_estw'th of boosting iteratn)
    nu.start=nu.start, # named list of the shrinkage rate, for different shrinkage per component (named the same as in formula)
    nu_search_steps = 7, # how many steps to take to find an optimal nu
    offsets=NA, # named list of start values per component (named the same as in formula) NA = MLE of phi(dot)p(dot)
    add.intercept=TRUE, # option to automatically add an intercept variable called 'interc'
    id = NULL, # optional vector of IDs to identify rows in data with ch data 
    N_bootstrap=N_bootstrap,# number of bootstrap iteration
    mc.cores=mc.cores, # package(parallel): parallelize the bootstrap runs, number of cores 
    bootstrap_weights=bootstrap_weights, # option to patch in your own weights,
    return_all_cv_models=FALSE) #  return ALL the cvmods (not a good idea); otherwise, just return the CV runs for the optimal nu (def

print(cv.fake.PLS$nu.optimization.summary) # look at bestm, cvrisk. We want bestm and nu to optimize cvrisk

bestm.fake.PLS <- round(cv.fake.PLS$bestm$median) # best m
bestnu.fake.PLS <-  cv.fake.PLS$optimal.nu # best nu
# PLS fake final model for inference: run until bestm
mod.fake.PLS <- cjsboost(formula=formu.fake.PLS, ch.data=y, cov.data.wide=fake.dat.indiv, cov.data.array=fake.dat.array, mstop = bestm.fake.PLS, m_estw=m_estw, nu=bestnu.fake.PLS, add.intercept=TRUE)

# print which base-learners were selected most (per s and p)
print(mod.fake.PLS$summary)

# print the (shrunken) estimates of the coefficients
coefs.fake <- boost_coef(mod.fake.PLS)
print(coefs.fake[["s"]]) # never uses fake1,2,3,4,5
print(coefs.fake[["p"]]) # uses fake1,2,3,4, but its effects are shrunk to <0.001 (i.e., no effect)

# STABILITY SELECTION: lets see the selection probabilities of the fake covariates. Selection probabilities are the probability (per reulgarization parameter) that a covariate is selected by the algorithm. For univariate boosting, these have two uses: i) straight-forward inference in and of themselves (important covariates have high selection probabilities (>0.8-0.95), unimportant covariates have low selection probabilities (<0.8); ii) we can discard unimportant covariates and get a "model-selection consistent" estimator, finding a true "sparse" model (at least in univariate boosting).  See Meinhausen and Buhlmann (2008).
# notice that NONE of the covariates have high selection probabilities for p, and only "Flood" for survival
col_ <- c("interc"="grey","timefactor"="blue", "sex"="red","floods" = "orange","floodp" = "orange",fake1="grey",fake2="grey" ,fake3="grey" ,fake4="grey" ,fake5="grey")
png(filename="PLOT_stabilityselection.png",width=700,height=700,pointsize=18)
par(mfrow=c(2,1),bty="l",mar=c(3.5,3.5,1,0),mgp=c(1.9,0.8,0));
for(cp_ in c("s","p")){
    # plot
    plot(c(1,mstop.cv),c(0,1),xlab = "m (boosting iteration)", ylab = "selection probabilities",typ="n",main=list(s=expression(phi),p="p")[[cp_]]) # blank plot
    abline(v = bestm.fake.PLS,lwd=1,lty=2); text(x=bestm.fake.PLS,y=0.5,labels=expression(m["cv"]),srt=90,adj=c(0,1)) # m optimal
    # add each stability line 
    for(j in 1:ncol(cv.fake.PLS$stabilsel[[cp_]])){
        cov.name=colnames(cv.fake.PLS$stabilsel[[cp_]])[j]
        lines(1:nrow(cv.fake.PLS$stabilsel[[cp_]]), cv.fake.PLS$stabilsel[[cp_]][,j],col=col_[[cov.name]])
    }
}
# add the legend
legend(x="topright",legend=c("time","sex","flood","fake"), col=c("time"="blue","sex"="red","flood" = "orange","fake"="grey"), lwd=1,lty=1,bty="n",cex=0.8)
dev.off()

# Stability selection: calculate (Approximate) Posterior Inclusion Probabilties. We can use these posterior probabilities for straightforward inference
inclusion.probs <- lapply(cv.fake.PLS$stabilsel, colMeans)
print(inclusion.probs$s) # ... only FLood has high inclusion probability
print(inclusion.probs$p) # safely discard all of these covariates

# RMark: how does MLE do with FAKE covariates?
# compare the CJSboost to MLE (in mark)
if("RMark"%in%row.names(installed.packages())){
    keep = which(dipper$ch!="0000001") # discard observations whose first capture is at t=T
    fake.dipper <- cbind(dipper[keep,],fake.dat.indiv[,c("fake1","fake2","fake3")])
    fake.processed=process.data(fake.dipper,groups=("sex"))
    fake.ddl=make.design.data(fake.processed)
    # RMark way of adding FLood covariate
    fake.ddl$Phi$Flood=0; fake.ddl$Phi$Flood[fake.ddl$Phi$time==2 | fake.ddl$Phi$time==3]=1; fake.ddl$p$Flood=0; fake.ddl$p$Flood[fake.ddl$p$time==3]=1
    # add time-varying fake 4&5 to the Mark design matrix (pain in the ass)
    fake.ddl.Phi <- merge(x=fake.ddl$Phi,y=cbind(time=0:6,fake.dat.array[1,,c("fake4","fake5")]),by="time")
    fake.ddl.p <- merge(x=fake.ddl$p,y=cbind(time=1:7,fake.dat.array[1,,c("fake4","fake5")]),by="time")    
    fake.ddl$Phi <- fake.ddl.Phi[order(fake.ddl.Phi$model.index),]
    fake.ddl$p <- fake.ddl.p[order(fake.ddl.p$model.index),]    
    mle.fake <- mark(data=fake.processed,ddl=fake.ddl,model="CJS",model.parameters=list(Phi=list(formula=~Flood*sex+fake1+fake2+fake3+fake4+fake5),p=list(formula=~Flood*sex+fake1+fake2+fake3+fake4+fake5)),delete=TRUE,retry=30)
    # poor convergence
    mle.fake$results$beta # incredibly high coefficients on FAKE covariates
    mle.fake$results$AICc # lower AICc than...  
    mle.model$results$AICc # ... pht(t x sex)p(t x sex)
}
# DONE part 6

####################################################################################################
# PART 7: Extreme high-dimensional example: simulated data
setwd("~/Documents/school/Duke/machinelearn/HMMboost")
library(Rcpp) # for C++ cjsboost functions
library(RcppArmadillo) # for C++ cjsboost functions
library(inline) # for C++ cjsboost functions
library(boot) # logit command
library(mboost) #stupid base-learners for boosting
library(parallel) # need for parallel processing during the bootstrap-runs
library(party) # (optional) conditional inference trees (for "boosted regression trees")
#source("R_CJSboost_SOURCE.R") # import CJSBoost functions (may take several seconds)
#source("R_simulatedata_SOURCE.R") # import simulation functions ( optional)
source("../markboost/EM/R_CJSboost_SOURCE.R") # import CJSBoost functions (may take several seconds)
source("../markboost/R_simcjsdataset_SOURCE.R") # import stupid simulation functions ( optional)
mc.cores = 2

T = 10 # number of capture periods
N.cont = 18 # 18 continuous covariates (individual level)
N.factor = 3 # 2 categorical covariates (individual level)
n.target = sample(200:300,1) # number of observed individuals

set.seed(9444)
# this is a SPARSE simulation: only 3 covariates will be truely influential, all other covariates are NOT 
sim <- sim.cjs(T=T,n.target=n.target,n.start=round(n.target*2.4),N.cont=N.cont,N.factor=N.factor, time.as.factor=TRUE,method_control=list(method="sparse",betanorm=list(p=3,s=3), taper_decay=list(p=c(a=0.3,0.8),s=c(a=0.3,0.8)), sparse_Ntruecov=list(p=3,s=3)),nl_control=list(nonlinear=FALSE,n.time.interactions=0,cor_strength=4.8),print_=FALSE,add.t.at.zero=TRUE,intercept_range=list(p=c(min=0.4,max=0.65), s=c(min=0.55,max=0.85)))
# looksig.effs[[1]] at the covariate data
head(sim$cov.data.wide)
# look at which covariates are actually significant
sim$sig.effs[[1]]
sim$sig.effs[[2]]

# formula for Penalized Least Squares base-learners
formu.highdim.PLS <- list(
    s = ~ bols(interc,intercept=FALSE) + bols(timefactor,df=2) + bols(a,df=2)+bols(b,df=2)+bols(c,df=2)+bols(d,df=2)+bols(e,df=2)+bols(f,df=2)+bols(g,df=2)+bols(h,df=2)+bols(i,df=2)+bols(j,df=2)+bols(k,df=2)+bols(l,df=2)+bols(m,df=2)+bols(n,df=2)+bols(o,df=2)+bols(p,df=2)+bols(q,df=2)+bols(r,df=2)+bols(s,df=2)+bols(t,df=2)+bols(u,df=2),
    p = ~ bols(interc,intercept=FALSE) + bols(timefactor,df=2) + bols(a,df=2)+bols(b,df=2)+bols(c,df=2)+bols(d,df=2)+bols(e,df=2)+bols(f,df=2)+bols(g,df=2)+bols(h,df=2)+bols(i,df=2)+bols(j,df=2)+bols(k,df=2)+bols(l,df=2)+bols(m,df=2)+bols(n,df=2)+bols(o,df=2)+bols(p,df=2)+bols(q,df=2)+bols(r,df=2)+bols(s,df=2)+bols(t,df=2)+bols(u,df=2)
) 

# formula for Conditional Inference Trees

# make bootstrap weights (with an eye on the factor covariates). The method='constrainedboot' ensures that each bootstrap sample includes at least one of each of each factor's categories. (i.e., we can't train a model on a subset of the categories, then predict on newdata with a different set of categories. For a small number of categories, the subsampling is approximately a bootstrap.
N_bootstrap=50 # in paper, I use 70
bootstrap_labels = sim$cov.data.wide[,c("s","t","u")]
bootstrap = subsampF(labels=bootstrap_labels, ntimes=N_bootstrap, oobfraction=1/3,method="constrainedboot") # subsampling function 
bootstrap_weights <- lapply(bootstrap, function(x) x$inbag) # weights used for training: holdout set are the zeros

# Hyperparameters tuning: we must optimize the hyperparameters 'nu' and 'mstop'. mstop is the total number of boosting iterations. nu is the shrinkage rate. The following function runs NU.STEP bootstrap-validations trying to find a reasonable pair of 'nu' for survival and 'p'. Then we tune 'mstop'
nu.start = list(s=0.2,p=0.2) # initial shrinkage rates (named list)
mstop.cv = 2500 # stopping iteration (should be overly generous)
m_estw <- 2 # integer, how often to re-calculate the E-step? (1-3) smaller is more computationally expensive
mc.cores = 10 # number of threads for parallel processing; used in parallel::mcLapply

# BOOTSTRAP-VALIDATION: for PLS baselearners
cv.highdim.PLS <- cjsboost_hyperparam( formula=formu.highdim.PLS, ch.data=sim$ch.data, cov.data.wide=sim$cov.data.wide, cov.data.array=NULL, mstop = mstop.cv, m_estw=m_estw, nu.start=nu.start, nu_search_steps = 7, offsets=NA, add.intercept=TRUE, id = NULL, N_bootstrap=N_bootstrap, mc.cores=mc.cores, bootstrap_weights=bootstrap_weights, return_all_cv_models=FALSE)
print(cv.highdim.PLS$nu.optimization.summary) # look at bestm, cvrisk. We want bestm and nu to optimize cvrisk
# ... having optimized nu and mstop, run final PLS high-dim model
bestm.PLS <- round(cv.highdim.PLS$bestm$median); bestnu.PLS <-  cv.highdim.PLS$optimal.nu # best nu
mod.highdim.PLS <- cjsboost(formula=formu.highdim.PLS, ch.data=sim$ch.data, cov.data.wide=sim$cov.data.wide, cov.data.array=NULL, mstop = bestm.PLS, m_estw=m_estw, nu=bestnu.PLS, add.intercept=TRUE)
# PLS: done model

# look at the final coefficients: compare to the truely influential effects
coefs <- boost_coef(mod.highdim.PLS)
print(sim$sig.effs$s) # which are truely influential for survival
print(sim$beta$s) # covariate true coefficients
print(coefs$s) # estimated
print(sim$sig.effs$p) # which are truely influential for capture-prob
print(sim$beta$p) # covariate true coefficients
print(coefs$p) # estimated
# calculate the (approximate) posterior inclusion probabilities
inclusion.probs = lapply(cv.highdim.PLS$stabilsel, colMeans) #
print(inclusion.probs[["s"]]) # vs. sim$sig.effs[["s"]]
print(which(inclusion.probs[["s"]]>0.8)) # vs. sim$sig.effs[["s"]]
print(inclusion.probs[["p"]]) # vs. sim$sig.effs[["p"]]
print(which(inclusion.probs[["p"]]>0.8)) # vs. sim$sig.effs[["p"]]
# the recommended threshold is ~ 0.8-0.95

# PLOT the regression cofficients (marginal values)
true.betas <- sim$beta # true coefficients
true.betas <- list(s = true.betas[["s"]][which(names(true.betas[["s"]])!="interc")], p =true.betas[["p"]][which(names(true.betas[["p"]])!="interc")]) # discard intercept
png(filename="PLOT_highdim_coef.png",width=700,height=700,pointsize=18)
  par(mfrow=c(2,1),bty="l",mar=c(3.5,3.5,1.3,0),mgp=c(2.5,0.8,0));
  plot.coef.s <- unlist(lapply(coefs[["s"]][-1], function(x) x[which(row.names(x)!="(Intercept)"),]))
  plot.coef.p <- unlist(lapply(coefs[["p"]][-1], function(x) x[which(row.names(x)!="(Intercept)"),]))
  barplot(true.betas[["s"]],las=2,col="green",xlab="",ylab="coefficients' marginal values",main=expression(beta[phi]~"Regression coefficients for"~phi),xpd=FALSE);#box(bty="l") # true values
  barplot(plot.coef.s,las=2,add=TRUE,names.arg="",col="purple",density=20); # true values
  legend(x="topleft",legend=c("true (simulated)","estimated"),fill=c("green","purple"),density=c(NA,20),cex=1.2,bty="n")
  barplot(true.betas[["p"]],las=2,col="green",xlab="covariates",ylab="coefficients' marginal values",main=expression(beta[p]~"Regression coefficients for"~p),xpd=FALSE);#box() # true values
  barplot(plot.coef.p,las=2,add=TRUE,names.arg="",col="purple",density=20) # true values
dev.off()

# PLOT compare the true processes vs. the estimated processes (each dot is an individual i at time t)
png(filename="PLOT_highdim_processes.png",width=700,height=700,pointsize=18)
  par(mfrow=c(2,1),bty="l",mar=c(3.5,3.5,1,0),mgp=c(1.9,0.8,0));
  plot(sim$true.wide[["s"]],mod.highdim.PLS$fit[["s"]],xlab=expression("true"~phi[list(i,t)]),ylab=expression("estimated"~hat(phi)[list(i,t)]),main="compare individuals' estimated and true survival"); abline(0,1) # plot survival (true vs estimated)
  plot(sim$true.wide[["p"]],mod.highdim.PLS$fit[["p"]],xlab=expression("true"~p[list(i,t)]),ylab=expression("estimated"~hat(p)[list(i,t)]),main="compare individuals' estimated and true capture prob"); abline(0,1) # plot capture probability (true vs estimated)
dev.off()

# plot the PLS stability selection pathways: which covariates have low stability selection <<0.8-0.95 are probably not that important
png(filename="PLOT_highdim_stability_PLS.png",width=700,height=700,pointsize=18)
  par(mfrow=c(2,1),bty="l",mar=c(3.5,3.5,1,0),mgp=c(1.9,0.8,0));
  for(cp_ in c("s","p")){
    col_ <- rbind(interc=0,timefactor=3,sim$sig.effs[[cp_]]+1) # significiant ones in red, others in blue)
    # plot
    plot(c(1,mstop.cv),c(0,1),xlab = "m (boosting iteration)", ylab = "selection probabilities",typ="n",main=list(s=expression("Stability selection paths for "~phi~"(PLS base-learners)"),p=expression("Stability selection paths for "~p~"(PLS base-learners)"))[[cp_]]) # blank plot
    abline(v = bestm.PLS,lwd=1,lty=2); text(x=bestm.PLS,y=0.5,labels=expression(m["cv"]),srt=90,adj=c(0,1)) # m optimal
    # add each stability line 
    for(j in 1:ncol(cv.highdim.PLS$stabilsel[[cp_]])){
        cov.name=colnames(cv.highdim.PLS$stabilsel[[cp_]])[j]
        lines(1:nrow(cv.highdim.PLS$stabilsel[[cp_]]), cv.highdim.PLS$stabilsel[[cp_]][,j],col=col_[cov.name,1])
    }
    legend(x="topright",legend=c("unimportant","important","theta(t)"), col=c(1,2,3),lwd=1,lty=1,bty="n",cex=0.7,ncol=1)
  }
dev.off()

# calculate the (approximate) posterior inclusion probabilities

# Boosting: Lets see how well conditional inference trees (boosted regression trees) do at finding the important covariates in a high-dimensional setting. Theorectically, it should do worse than PLS, because the true model is strictly logit-linear... but trees would do better if there were possible non-linearity or interactions
formu.highdim.TREES <- list(
    s = ~ bols(interc,intercept=FALSE) + btree(timefactor,a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,tree_controls=party::ctree_control(stump=TRUE,mincriterion=0,savesplitstats=FALSE)),
    p = ~ bols(interc,intercept=FALSE) + btree(timefactor,a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,tree_controls=party::ctree_control(stump=TRUE,mincriterion=0,savesplitstats=FALSE))
) 

# TREES bootstrap tuning of mstop and nu
nu.start = list(s=0.2,p=0.2) # initial shrinkage rates (named list)
mstop.cv = 2500 # stopping iteration (should be overly generous)
m_estw <- 2 # integer, how often to re-calculate the E-step? (1-3) smaller is more computationally expensive
mc.cores = 10 # number of threads for parallel processing; used in parallel::mcLapply
cv.highdim.TREES <- cjsboost_hyperparam( formula=formu.highdim.TREES, ch.data=sim$ch.data, cov.data.wide=sim$cov.data.wide, cov.data.array=NULL, mstop = mstop.cv, m_estw=m_estw, nu.start=nu.start, nu_search_steps = 7, offsets=NA, add.intercept=TRUE, id = NULL, N_bootstrap=N_bootstrap, mc.cores=mc.cores, bootstrap_weights=bootstrap_weights, return_all_cv_models=FALSE)
print(cv.highdim.TREES$nu.optimization.summary) # look at bestm, cvrisk. We want bestm and nu to optimize cvrisk
# ... having optimized nu and mstop, run final TREES high-dim model
bestm.TREES <- round(cv.highdim.TREES$bestm$median); bestnu.TREES <-  cv.highdim.TREES$optimal.nu # best nu
mod.highdim.TREES <- cjsboost(formula=formu.highdim.TREES, ch.data=sim$ch.data, cov.data.wide=sim$cov.data.wide, cov.data.array=NULL, mstop = bestm.TREES, m_estw=m_estw, nu=bestnu.TREES, add.intercept=TRUE)

# PLOT: TREES vs. PLS fits
png(filename="PLOT_highdim_PLSvsTREES.png",width=700,height=700,pointsize=18)
  par(mfrow=c(2,1),bty="l",mar=c(3.5,3.5,1,0),mgp=c(1.9,0.8,0));
  # survival
  plot(sim$true.wide[["s"]],mod.highdim.PLS$fit[["s"]],xlab=expression("true"~phi[list(i,t)]),ylab=expression("estimated"~hat(phi)[list(i,t)]),main="compare PLS vs TREES (survival)",col="purple"); abline(0,1,lwd=2) # plot survival (true vs estimated)
  points(sim$true.wide[["s"]],mod.highdim.TREES$fit[["s"]],col="orange",pch=2) # ... add the trees
  # add trendlines
  abline(lm(pls~true,data=data.frame(true=as.numeric(sim$true.wide[["s"]]),pls=as.numeric(mod.highdim.PLS$fit[["s"]]))),col="purple",lty=2)
  abline(lm(tre~true,data=data.frame(true=as.numeric(sim$true.wide[["s"]]),tre=as.numeric(mod.highdim.TREES$fit[["s"]]))),col="orange",lty=2)
# capture prob
  plot(sim$true.wide[["p"]],mod.highdim.PLS$fit[["p"]],xlab=expression("true"~p[list(i,t)]),ylab=expression("estimated"~hat(p)[list(i,t)]),main="compare PLS vs TREES (capture prob)",col="purple"); abline(0,1,lwd=2) # plot capture probability (true vs estimated)
  points(sim$true.wide[["p"]],mod.highdim.TREES$fit[["p"]],col="orange",pch=2) # ... add the trees
  abline(lm(pls~true,data=data.frame(true=as.numeric(sim$true.wide[["p"]]),pls=as.numeric(mod.highdim.PLS$fit[["p"]]))),col="purple",lty=2)
  abline(lm(tre~true,data=data.frame(true=as.numeric(sim$true.wide[["p"]]),tre=as.numeric(mod.highdim.TREES$fit[["p"]]))),col="orange",lty=2)
  legend(x="bottomright",bty="n",legend=c("PLS","Trees"),col=c("purple","orange"),pch=c(1,2))
# add trendlines
dev.off()

# PLOT the stability selection pathways for the TREES model
# plot the PLS stability selection pathways: which covariates have low stability selection <<0.8-0.95 are probably not that important
png(filename="PLOT_highdim_stability_TREES.png",width=700,height=700,pointsize=18)
  par(mfrow=c(2,1),bty="l",mar=c(3.5,3.5,1,0),mgp=c(1.9,0.8,0));
  for(cp_ in c("s","p")){
    col_ <- rbind(interc=0,timefactor=3,sim$sig.effs[[cp_]]+1) # significiant ones in red, others in blue)
    # plot
    plot(c(1,mstop.cv),c(0,1),xlab = "m (boosting iteration)", ylab = "selection probabilities",typ="n",main=list(s=expression("Stability selection paths for "~phi~"(CART base-learners)"),p=expression("Stability selection paths for "~p~"(CART base-learners)"))[[cp_]]) # blank plot
    abline(v = bestm.TREES,lwd=1,lty=2); text(x=bestm.TREES,y=0.5,labels=expression(m["cv"]),srt=90,adj=c(0,1)) # m optimal
    # add each stability line 
    for(j in 1:ncol(cv.highdim.TREES$stabilsel[[cp_]])){
        cov.name=colnames(cv.highdim.TREES$stabilsel[[cp_]])[j]
        lines(1:nrow(cv.highdim.TREES$stabilsel[[cp_]]), cv.highdim.TREES$stabilsel[[cp_]][,j],col=col_[cov.name,1])
    }
    legend(x="topright",legend=c("unimportant","important","theta(t)"), col=c(1,2,3),lwd=1,lty=1,bty="n",cex=0.7,ncol=1)
  }
dev.off()

# DONE part 7

###############################################################################################################
# PART 8: CJSboost-EM vs CJSboost-MC: strictly academic
# this part won't be of much practical interest to anyone. This merely shows the similarity between estimates between the CJSboost-Expectation-Maximization Algorithm and the Monte-Carlo algorithm. The EM is faster, and what is recommended. But, the MC could be useful for other capture-recapture systems. It is a proof of concept.In this demonstration, we simulate a high-dimensional dataset. 
# This assumes you have already run PART 8: highdimensional example with PLS
# ... here, we won't re-optimize nu and mstop.... although if this were anything but a demonstration, you would. Because we already optimized nu and mstop for the CJSboost-EM PLS model in Part 7, this CJSboost-MC model should have the same regularization parameters

m_estw.MC <- 20 # number of draws from posterior of z (to approximate the gradient)
bestm.MC <- bestm.PLS*m_estw.MC # here, m is 'number of boosting iterations' x 'm_estw.MC'
nu.MC <- lapply(bestnu.PLS, prod, 1/m_estw.MC) # need to lower the shrinkage rate by 1/20, so that each draw from the posterior on contributes a tiny amount to the overall fit
mod.MonteCarlo <- cjsboost.stochastic(formula=formu.highdim.PLS, ch.data=sim$ch.data, cov.data.wide=sim$cov.data.wide, cov.data.array=NULL, mstop = bestm.MC, m_estw=m_estw.MC, nu=nu.MC, offsets=NA,add.intercept=TRUE)
# ... this obviously will take a lot longer... x20 longer. It takes such a long time because each boosting iteration now has x20 gradient estimates and x20 fitting base-learners to the gradient

# Plot the CJSboost-EM results vs. the CJSboost-MC results (should be approx. the same).
png(filename="PLOT_MCvsEM.png",width=700,height=700,pointsize=18)
  par(mfrow=c(2,1),bty="l",mar=c(3.5,3.5,1,0),mgp=c(1.9,0.8,0));
  # survival
  plot(mod.highdim.PLS$fit[["s"]],mod.MonteCarlo$fit[["s"]],xlab=expression("EM"~hat(phi)[list(i,t)]),ylab=expression("MC"~hat(phi)[list(i,t)]),main="EM vs MC (survival)",col="purple"); abline(0,1,lwd=2) # plot survival (EM vs MC)
  plot(mod.highdim.PLS$fit[["p"]],mod.MonteCarlo$fit[["p"]],xlab=expression("EM"~hat(p)[list(i,t)]),ylab=expression("MC"~hat(p)[list(i,t)]),main="EM vs MC (capture-prob)",col="purple"); abline(0,1,lwd=2) # plot capture-prob (EM vs MC)
dev.off()
# 
